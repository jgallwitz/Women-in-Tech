# -*- coding: utf-8 -*-
"""WIT_Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sMUmCMSiSB5ssfaP01X49TVuedei5REg
"""

# import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
from sklearn.preprocessing import StandardScaler

# read in CSV file and view dataset
jobs_survey = pd.read_csv('/content/job_survey.csv')
jobs_survey.head()

# dropping all rows that are outside of the US
jobs_survey = jobs_survey.drop(jobs_survey[jobs_survey['Country'] != 'United States of America'].index)

# dropping all NonDev employees
jobs_survey = jobs_survey.drop(jobs_survey[jobs_survey['MainBranch'] != 'Dev'].index)

# drop all nonbinary respondents since I want to compare men and women
jobs_survey = jobs_survey.drop(jobs_survey[jobs_survey['Gender'] == 'NonBinary'].index)

# reindexing
jobs_survey = jobs_survey.reset_index(drop=True)

# drop specified columns
jobs_survey = jobs_survey.drop(columns = ['Unnamed: 0', 'Accessibility','HaveWorkedWith'])

jobs_survey_clustering = jobs_survey.drop(columns = ['Age', 'EdLevel','Employment','Employed','MentalHealth','MainBranch','Country'])
jobs_survey_clustering.head()

# drop the label and save it for later
X = jobs_survey_clustering.drop('Gender', axis = 1)
y = jobs_survey_clustering['Gender']

# scale the features using StandardScaler
features = jobs_survey_clustering[['YearsCode', 'YearsCodePro', 'PreviousSalary', 'ComputerSkills']]
scaled_features = StandardScaler().fit_transform(features)

scaled_features

# perform PCA on the clean and scaled dataset
pca = PCA()

principalComponents = pca.fit_transform(scaled_features)
principalDf = pd.DataFrame(data = principalComponents)


# propogate principal components to a df
principalDf = pd.DataFrame(data = principalComponents
                           , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4'])

# print PC df
principalDf

# drop PC4 so we are left with a 3D dataset
jobs_survey_clustering_reduced = principalDf.drop(columns = ['principal component 4'])
jobs_survey_clustering_reduced.head()

# import necessary libraries
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from mpl_toolkits.mplot3d import Axes3D

# add Gender label back in for plotting purposes
jobs_survey_clustering_reduced['Gender'] = y

# map gender label to numeric ONLY for plotting purposes
jobs_survey_clustering_reduced['Gender_numeric'] = jobs_survey_clustering_reduced['Gender'].map({'Man': 0, 'Woman': 1})

#assign colors to labels
colors = np.where(jobs_survey_clustering_reduced['Gender_numeric'] == 1, 'b', 'r')

# define range for clusters to test
range_n_clusters = [2, 3, 4]

# initialize for loop
for n_clusters in range_n_clusters:
  # initialize kmeans
  clusterer = KMeans(n_clusters=n_clusters, random_state=10)
  # perform kmeans on 3 PCs
  cluster_labels = clusterer.fit_predict(jobs_survey_clustering_reduced[['principal component 1', 'principal component 2', 'principal component 3']])

  # calculate silhouette score
  silhouette_avg = silhouette_score(jobs_survey_clustering_reduced[['principal component 1', 'principal component 2', 'principal component 3']], cluster_labels)
  # print score for n clusters
  print(f"For n_clusters = {n_clusters}, the average silhouette_score is: {silhouette_avg}")

  # plot each n cluster graph
  fig = plt.figure(figsize=(10, 8))
  ax = fig.add_subplot(111, projection='3d')

  scatter = ax.scatter(
      jobs_survey_clustering_reduced['principal component 1'],
      jobs_survey_clustering_reduced['principal component 2'],
      jobs_survey_clustering_reduced['principal component 3'],
      c=colors, alpha=0.01)
  # add centroids
  centers = clusterer.cluster_centers_
  ax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], c='black', marker='X', s=300, label='Centroids')

  # set labels
  ax.set_title(f'3D KMeans Clustering with n_clusters = {n_clusters}')
  ax.set_xlabel('Principal Component 1')
  ax.set_ylabel('Principal Component 2')
  ax.set_zlabel('Principal Component 3')

  ax.legend()

  plt.show()

# import necessary package
from scipy.cluster.hierarchy import dendrogram, linkage

# define data to be used for clustering
numeric_data = jobs_survey_clustering_reduced[['principal component 1', 'principal component 2', 'principal component 3']]

# create sample as there was too much data to use all of it for this type of clustering
sampled_data = numeric_data.sample(n=100, random_state=1)
# execute hierarchal clustering
Z = linkage(sampled_data, method='ward')

# plot results using a dendrogram
plt.figure(figsize=(12, 8))
dendrogram(Z, leaf_rotation=90, leaf_font_size=12)
plt.title('Hierarchical Clustering Dendrogram')
plt.show()

# import necessary library
from sklearn.cluster import DBSCAN

# set data to be used for clustering
numeric_data = jobs_survey_clustering_reduced.select_dtypes(include=['number'])

# set DBSCAN parameters
eps_value = 0.5
min_samples_value = 6

# initialize and perofrm DBSCAN
dbscan = DBSCAN(eps=eps_value, min_samples=min_samples_value)
dbscan_labels = dbscan.fit_predict(numeric_data)

# plot results
plt.figure(figsize=(12, 8))
unique_labels = set(dbscan_labels)
colors = [plt.cm.Spectral(float(i) / len(unique_labels)) for i in range(len(unique_labels))]

# loop through each label and plot corresponding points
for k, col in zip(unique_labels, colors):
  if k == -1:
    col = 'k'
  class_member_mask = (dbscan_labels == k)
  plt.scatter(jobs_survey_clustering_reduced.iloc[class_member_mask, 0],
              jobs_survey_clustering_reduced.iloc[class_member_mask, 1],
              color=col,
              label=k,
              alpha = 0.4)

plt.title('DBSCAN Clustering on jobs_survey_clustering_reduced')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()